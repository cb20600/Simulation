# from utils.pose_capture import simulate_and_capture_scene
from utils.yolo_utils import detect_fruits
from utils.sam2_utils import segment_with_sam2
from utils.coordinate import annotate_and_get_3d_coords
from utils.save_json import save_json_from_detection
from utils.rotate import extract_grasp_infos, simple_quaternion_from_angle
from utils.desk4 import create_scene
from GPT import parser
from pose_capture import capture

import os
import numpy as np
from openai import OpenAI
from dotenv import load_dotenv

if __name__ == "__main__": 
    '''
    1. è°ƒç”¨LLMè¯»å–ç”¨æˆ·æŒ‡ä»¤(éœ€è¦æ¨¡ç³ŠåŠŸèƒ½è¯†åˆ«æ‹¼å†™é”™è¯¯ç­‰): LLMè¾“å‡ºå­ä»»åŠ¡åºåˆ—
    2. åˆ›å»ºåœºæ™¯,æ‰§è¡Œåœºæ™¯å…¨è§ˆè¿åŠ¨,è°ƒç”¨æ‘„åƒå¤´æ‹æ‘„æ¡Œé¢å›¾åƒ: è¾“å‡ºä¸ºRGB-Då›¾åƒ
    3. ä½¿ç”¨yolo_utils.pyè¿›è¡Œè¯†åˆ«: image_rgb, boxes, yolo_centers, class_ids
    4. SAM2å¤„ç†(sam2_utils.py): out_masks, centers, save_path. è®¡ç®—ç‰©ä½“ä¸»è½´å‰¯è½´æ–¹å‘, ç»™å‡ºå¯¹åº”çš„quatå€¼(rotate.py)
    5. è°ƒç”¨corrdinate.pyåæŠ•å½±ç‚¹äº‘è®¡ç®—3Dä½ç½®, å¹¶ä½¿ç”¨save_json.pyä¿å­˜ä¸ºjsonæ–‡ä»¶
    6. LLMè¯»å–jsonæ–‡ä»¶å†…å®¹, å¹¶æ ¹æ®å­ä»»åŠ¡åºåˆ—ï¼Œç”Ÿæˆè¯¦ç»†çš„è½¨è¿¹ç‚¹å’Œä»»åŠ¡
    7. è°ƒç”¨ä»¿çœŸæ§åˆ¶å™¨æ‰§è¡Œè½¨è¿¹ç‚¹åºåˆ—, æ ¹æ®quatå€¼å’Œç‰©ä½“å®½åº¦è°ƒæ•´å¤¹çˆªè§’åº¦å’Œå¼€åˆå¤§å°
    '''
    enable_gui = True
    yolo_model_path = "checkpoints/best_yolo.pt"
    sam2_model_path = "checkpoints/sam2_b.pt"
    path_base = "imgs/sim_fruit_from_camera"
    img_path = f"{path_base}.png"
    npz_path = f"{path_base}.npz"
    json_path = f"{path_base}.json"

    # ======================= Step 1: LLM Command Parse =======================
    # åˆå§‹åŒ– API
    load_dotenv()
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    # æ¥æ”¶ç”¨æˆ·æŒ‡ä»¤
    user_input = input("Enter your robot instruction: ")
    task_description = parser(user_input)

    # ======================= Step 2: Capture Pose =======================
    capture(img_path, npz_path)

    # ======================= Step 3: YOLO Detection =======================
    image_rgb, boxes, yolo_centers, class_ids, yolo_output_path = detect_fruits(img_path, yolo_model_path)
    if len(boxes) == 0:
        print("âŒ No objects detected by YOLO. Exit!")
        exit()

    # ======================= Step 4: SAM2 Segmentation & Grasp Info Extraction ====================
    masks, sam_centers, result_path = segment_with_sam2(
        image_rgb=image_rgb,
        boxes=boxes,
        sam_model_path=sam2_model_path,
        input_image_path=img_path
    )
    grasp_infos, grasp_img_path = extract_grasp_infos(
        image_rgb=image_rgb,
        masks=masks,
        pixel_to_meter=None,
        output_path=f"{path_base}_grasp_visual.png"
    )

    quat = [simple_quaternion_from_angle(info["angle_deg"]) for info in grasp_infos]
    widths = [info["width"] for info in grasp_infos]

    # print("âœ… æŠ“å–ä¿¡æ¯ï¼š")
    # for info, quat in zip(grasp_infos, quat):
    #     print(f"# {info['index']}: angle={info['angle_deg']:.2f}Â°, width={info['width']:.4f}m, quat={quat}")

    # ======================= Step 5: 3D Coordinate Projection =============
    coords_3d = annotate_and_get_3d_coords(
        image_path=img_path,
        npz_path=npz_path,
        pixel_points=sam_centers
    )

    for i, coord in enumerate(coords_3d):
        if coord is not None:
            print(f"ğŸ§­ Object #{i} 3D position: [{coord[0]:.3f}, {coord[1]:.3f}, {coord[2]:.3f}]")
        else:
            print(f"âš ï¸ Object #{i} has no valid 3D coordinate")

    # ======================= Step 6: Save JSON ============================
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ YOLO ç±»åˆ«æ˜ å°„
    if class_ids is not None and yolo_model_path:
        save_json_from_detection(
            class_ids=class_ids,
            centers=sam_centers,
            coords_3d=coords_3d,
            widths=widths,
            quaternions=quat,
            yolo_path=yolo_model_path,
            output_path=json_path
        )
    else:
        # è‡ªå®šä¹‰ç±»åˆ«æ˜ å°„ï¼šobject_0, object_1 ...
        custom_names = {i: f"object_{i}" for i in range(len(sam_centers))}
        save_json_from_detection(
            class_ids=[i for i in range(len(sam_centers))],
            centers=sam_centers,
            coords_3d=coords_3d,
            widths=widths,
            quaternions=quat,
            class_names=custom_names,
            output_path=json_path
        )

        
'''
    # # ======================= Step 3: YOLO Detection =======================

    # # ä½¿ç”¨ä½ åŸæ¥çš„ YOLO æ£€æµ‹å‡½æ•°
    # image_rgb, boxes, yolo_centers, class_ids, yolo_output_path = detect_fruits(img_path, yolo_model_path)
    # if len(boxes) == 0:
    #     print("âŒ No objects detected by YOLO. Exit!")
    #     exit()

    # print("yoloä¸­å¿ƒç‚¹ï¼š", yolo_centers)

    # # ======================= Step 4: SAM2 Segmentation ====================
    # masks, sam_centers, result_path = segment_with_sam2(image_rgb, boxes, sam2_model_path, img_path)
    # print("SAM2ä¸­å¿ƒç‚¹ï¼š", sam_centers)

    # grasp_infos, yolo_img_path = extract_grasp_infos(
    # image_rgb=image_rgb,
    # masks=masks,
    # pixel_to_meter=0.0025,  # å¯é€‰ï¼šåƒç´ è½¬æ¢ä¸ºç±³çš„æ¯”ä¾‹
    # output_path="grasp_infos_visual.png"
    # )

    # # è¾“å‡ºç»“æœ
    # print("âœ… æŠ“å–ä¿¡æ¯å¦‚ä¸‹ï¼š")
    # for info in grasp_infos:
    #     print(info)
    # for info in grasp_infos:
    #     angle = info["angle_deg"]
    #     quat = grasp_angle_to_quaternion(angle)
    #     print(f"ç‰©ä½“ #{info['index']} å››å…ƒæ•°å§¿æ€: [{quat[0]:.2f}, {quat[1]:.2f}, {quat[2]:.2f}, {quat[3]:.2f}]")


    # print("âœ… SAM2 å¯è§†åŒ–å›¾å·²ä¿å­˜è‡³ï¼š", output_img_path)

    # masks, sam_centers, sam2_result = segment_with_sam2(
    #     image_rgb=image_rgb,
    #     boxes=boxes,
    #     sam_model_path=sam2_model_path,
    #     input_image_path=img_path
    # )

    # # Optional: extract grasp angles and quaternions
    # grasp_infos, grasp_img_path = extract_grasp_infos(
    #     image_rgb=image_rgb,
    #     masks=masks,
    #     pixel_to_meter=0.0025,
    #     output_path=img_path.replace(".png", "_grasp_infos.png")
    # )

    # quaternions = []
    # for info in grasp_infos:
    #     angle_deg = info["angle_deg"]
    #     quat = grasp_angle_to_quaternion(angle_deg)
    #     quaternions.append(quat)
    #     print(f"ğŸ” Object #{info['index']} grasp angle = {angle_deg:.2f}Â°, quaternion = {quat}")

    # # ======================= Step 5: 3D Coordinate Projection =============
    # coords_3d = annotate_and_get_3d_coords(
    #     image_path=img_path,
    #     npz_path=npz_path,
    #     pixel_points=sam_centers  # Prefer sam_centers for precision
    # )

    # # Print coordinates
    # for i, coord in enumerate(coords_3d):
    #     if coord is not None:
    #         print(f"ğŸ§­ Object #{i} 3D position: [{coord[0]:.3f}, {coord[1]:.3f}, {coord[2]:.3f}]")
    #     else:
    #         print(f"âš ï¸ Object #{i} has no valid 3D coordinate")

    # from utils.save_json import save_json_from_detection

    # save_json_from_detection(
    #     yolo_path=yolo_model_path,
    #     class_ids=class_ids,
    #     centers=sam_centers,
    #     coords_3d=coords_3d,
    #     widths=[info["width_m"] for info in grasp_infos],
    #     quaternions=quaternions,
    #     # gripper_opens=[compute_gripper_open_close(info["width_m"])[0] for info in grasp_infos],
    #     # gripper_closes=[compute_gripper_open_close(info["width_m"])[1] for info in grasp_infos],
    #     # output_path=json_path
    # )
'''