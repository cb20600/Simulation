# from utils.pose_capture import simulate_and_capture_scene
from utils.yolo_utils import detect_fruits
from utils.sam2_utils import segment_with_sam2
from utils.coordinate import annotate_and_get_3d_coords
from utils.save_json import save_json_from_detection
from utils.rotate import extract_grasp_infos, grasp_angle_to_quaternion
from utils.desk4 import create_scene

import numpy as np

if __name__ == "__main__": 
    '''
    1. è°ƒç”¨LLMè¯»å–ç”¨æˆ·æŒ‡ä»¤(éœ€è¦æ¨¡ç³ŠåŠŸèƒ½è¯†åˆ«æ‹¼å†™é”™è¯¯ç­‰): LLMè¾“å‡ºå­ä»»åŠ¡åºåˆ—
    2. åˆ›å»ºåœºæ™¯,æ‰§è¡Œåœºæ™¯å…¨è§ˆè¿åŠ¨,è°ƒç”¨æ‘„åƒå¤´æ‹æ‘„æ¡Œé¢å›¾åƒ: è¾“å‡ºä¸ºRGB-Då›¾åƒ
    3. ä½¿ç”¨yolo_utils.pyè¿›è¡Œè¯†åˆ«: image_rgb, boxes, yolo_centers, class_ids
    4. SAM2å¤„ç†(sam2_utils.py): out_masks, centers, save_path. è®¡ç®—ç‰©ä½“ä¸»è½´å‰¯è½´æ–¹å‘, ç»™å‡ºå¯¹åº”çš„quatå€¼(rotate.py)
    5. è°ƒç”¨corrdinate.pyåæŠ•å½±ç‚¹äº‘è®¡ç®—3Dä½ç½®, å¹¶ä½¿ç”¨save_json.pyä¿å­˜ä¸ºjsonæ–‡ä»¶
    6. LLMè¯»å–jsonæ–‡ä»¶å†…å®¹, å¹¶æ ¹æ®å­ä»»åŠ¡åºåˆ—ï¼Œç”Ÿæˆè¯¦ç»†çš„è½¨è¿¹ç‚¹å’Œä»»åŠ¡
    7. è°ƒç”¨ä»¿çœŸæ§åˆ¶å™¨æ‰§è¡Œè½¨è¿¹ç‚¹åºåˆ—, æ ¹æ®quatå€¼å’Œç‰©ä½“å®½åº¦è°ƒæ•´å¤¹çˆªè§’åº¦å’Œå¼€åˆå¤§å°
    '''
    enable_gui = True
    yolo_model_path = "checkpoints/best_yolo.pt"
    sam2_model_path = "checkpoints/sam2_b.pt"


    path_base = "imgs/sim_fruit_from_camera"
    img_path = f"{path_base}.png"
    npz_path = f"{path_base}.npz"
    json_path = f"{path_base}.json"


    # ======================= Step 2: Capture Pose =======================
    scene, xarm7, fruits, bins, camera = create_scene(enable_gui)
    print("ç­‰å¾…ç‰©ç†ç³»ç»Ÿç¨³å®š...")

    for _ in range(100):
        scene.step()

    # lime = fruits["lime"]
    # print("lime:", lime.get_pos())
    jnt_names = [
        "joint1", "joint2", "joint3",
        "joint4", "joint5", "joint6", "joint7"
    ]
    dofs_idx = [xarm7.get_joint(name).dof_idx_local for name in jnt_names]

    # 2. PID & åŠ›çŸ©é™åˆ¶è®¾ç½®
    xarm7.set_dofs_kp(
        kp=np.array([4500, 4500, 3500, 3500, 2000, 2000, 2000]),
        dofs_idx_local=dofs_idx
    )
    xarm7.set_dofs_kv(
        kv=np.array([450, 450, 350, 350, 200, 200, 200]),
        dofs_idx_local=dofs_idx
    )
    xarm7.set_dofs_force_range(
        lower=np.array([-87, -87, -87, -87, -12, -12, -12]),
        upper=np.array([ 87,  87,  87,  87,  12,  12,  12]),
        dofs_idx_local=dofs_idx
    )

    # 3. å®šä¹‰åˆå§‹å’Œç›®æ ‡å…³èŠ‚è§’
    pos_init   = np.array([0, 0, 0, 1, 0, 0.5, 0])
    for _ in range(100):
        scene.step()

    # 6. é‡ç½®åˆ°åˆå§‹å§¿æ€
    for i in range(100):
        xarm7.set_dofs_position(pos_init, dofs_idx)
        scene.step()

    # if enable_gui==True:
    #     camera.render(rgb=True)

    # è®¾ç½®æœ«ç«¯æ‰§è¡Œå™¨ä¸ºå¤¹çˆªåŸºåº§
    end_effector = xarm7.get_link("xarm_gripper_base_link")
    # è®¾ç½®æ˜¾ç¤ºè¯¥ link çš„åæ ‡è½´

    end_pos = end_effector.get_pos().cpu().numpy()
    print("æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ï¼š",end_pos)
    
    base_pos = xarm7.get_link("xarm_gripper_base_link").get_pos()
    left_finger_pos = xarm7.get_link("left_finger").get_pos()
    right_finger_pos = xarm7.get_link("right_finger").get_pos()

    print("gripper_base_pos     = [{:.3f}, {:.3f}, {:.3f}]".format(*base_pos))
    print("left_finger_pos      = [{:.3f}, {:.3f}, {:.3f}]".format(*left_finger_pos))
    print("right_finger_pos     = [{:.3f}, {:.3f}, {:.3f}]".format(*right_finger_pos))

    grasp_point = (left_finger_pos + right_finger_pos) / 2
    gripper_offset = grasp_point - base_pos

    print("âš™ï¸ gripper_offset     = [{:.3f}, {:.3f}, {:.3f}]".format(*gripper_offset))

    # è®¾ç½®ç›®æ ‡ä½ç½®å’Œæ–¹å‘ï¼ˆç«–ç›´å‘ä¸‹ï¼‰
    target_pos = np.array([0.20, 0.0, 1])
    target_quat = np.array([0, -0.7071, 0.7071, 0])  # Zè½´æœä¸‹ï¼Œå§¿æ€å¯å¾®è°ƒ

    # ä½¿ç”¨é€†è¿åŠ¨å­¦æ±‚è§£ç›®æ ‡å§¿æ€çš„å…³èŠ‚ä½ç½®
    qpos = xarm7.inverse_kinematics(
        link=end_effector,
        pos=target_pos,
        quat=target_quat,
    )

    # è®¾ç½®å¤¹çˆªå¼ å¼€ï¼ˆå‡è®¾æœ«6ä¸ªè‡ªç”±åº¦æ˜¯å¤¹çˆªï¼‰
    qpos[7:] = 0.04

    # ç”Ÿæˆè¿åŠ¨è·¯å¾„
    path = xarm7.plan_path(qpos_goal=qpos, num_waypoints=200)

    # æ‰§è¡Œè·¯å¾„
    if enable_gui:
        for waypoint in path:
            xarm7.control_dofs_position(waypoint)
            scene.step()
            camera.render(rgb=True)
    else:
        for waypoint in path:
            xarm7.control_dofs_position(waypoint)
            scene.step()

    # control_xarm7(scene, xarm7, cam_attachment, camera)
    # print(camera.transform)
    for i in range(50):
        scene.step()
    print("âœ… æ§åˆ¶ç»“æŸ")

    if enable_gui:
        # æ‹æ‘„RGBå›¾åƒå¹¶ä¿å­˜
        rgb_img, _, _, _ = camera.render(rgb=True)
        import cv2
        rgb_bgr = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)
        cv2.imwrite(img_path, rgb_bgr)
        print(f"âœ… å›¾åƒå·²ä¿å­˜åˆ°: {img_path}")

        # è·å–ç‚¹äº‘å¹¶ä¿å­˜
        pointcloud, mask_idx = camera.render_pointcloud(world_frame=True)[:2]
        mask = np.zeros(pointcloud.shape[:2], dtype=bool)
        mask[mask_idx] = True
        np.savez(npz_path, pointcloud=pointcloud, mask=mask)
        print(f"âœ… ç‚¹äº‘å’Œæ©ç å·²ä¿å­˜åˆ°: {npz_path}")


    # ======================= Step 3: YOLO Detection =======================
    image_rgb, boxes, yolo_centers, class_ids, yolo_output_path = detect_fruits(img_path, yolo_model_path)
    if len(boxes) == 0:
        print("âŒ No objects detected by YOLO. Exit!")
        exit()

    # ======================= Step 4: SAM2 Segmentation ====================
    masks, sam_centers, result_path = segment_with_sam2(
        image_rgb=image_rgb,
        boxes=boxes,
        sam_model_path=sam2_model_path,
        input_image_path=img_path
    )

    # ======================= Step 5: Grasp Info Extraction ================
    grasp_infos, grasp_img_path = extract_grasp_infos(
        image_rgb=image_rgb,
        masks=masks,
        pixel_to_meter=0.0025,
        output_path=f"{path_base}_grasp_infos_visual.png"
    )

    quaternions = [grasp_angle_to_quaternion(info["angle_deg"]) for info in grasp_infos]
    widths = [info["width_m"] for info in grasp_infos]  # âœ… æ”¹ä¸º width_m

    print("âœ… æŠ“å–ä¿¡æ¯ï¼š")
    for info, quat in zip(grasp_infos, quaternions):
        print(f"# {info['index']}: angle={info['angle_deg']:.2f}Â°, width={info['width_m']:.4f}m, quat={quat}")

    # ======================= Step 6: 3D Coordinate Projection =============
    coords_3d = annotate_and_get_3d_coords(
        image_path=img_path,
        npz_path=npz_path,
        pixel_points=sam_centers
    )

    for i, coord in enumerate(coords_3d):
        if coord is not None:
            print(f"ğŸ§­ Object #{i} 3D position: [{coord[0]:.3f}, {coord[1]:.3f}, {coord[2]:.3f}]")
        else:
            print(f"âš ï¸ Object #{i} has no valid 3D coordinate")

    # ======================= Step 7: Save JSON ============================
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ YOLO ç±»åˆ«æ˜ å°„
    if class_ids is not None and yolo_model_path:
        save_json_from_detection(
            class_ids=class_ids,
            centers=sam_centers,
            coords_3d=coords_3d,
            widths=widths,
            quaternions=quaternions,
            yolo_path=yolo_model_path,
            output_path=json_path
        )
    else:
        # è‡ªå®šä¹‰ç±»åˆ«æ˜ å°„ï¼šobject_0, object_1 ...
        custom_names = {i: f"object_{i}" for i in range(len(sam_centers))}
        save_json_from_detection(
            class_ids=[i for i in range(len(sam_centers))],
            centers=sam_centers,
            coords_3d=coords_3d,
            widths=widths,
            quaternions=quaternions,
            class_names=custom_names,
            output_path=json_path
        )

    # # ======================= Step 3: YOLO Detection =======================


    # # ä½¿ç”¨ä½ åŸæ¥çš„ YOLO æ£€æµ‹å‡½æ•°
    # image_rgb, boxes, yolo_centers, class_ids, yolo_output_path = detect_fruits(img_path, yolo_model_path)
    # if len(boxes) == 0:
    #     print("âŒ No objects detected by YOLO. Exit!")
    #     exit()

    # print("yoloä¸­å¿ƒç‚¹ï¼š", yolo_centers)





    # # ======================= Step 4: SAM2 Segmentation ====================
    # masks, sam_centers, result_path = segment_with_sam2(image_rgb, boxes, sam2_model_path, img_path)
    # print("SAM2ä¸­å¿ƒç‚¹ï¼š", sam_centers)

    # grasp_infos, yolo_img_path = extract_grasp_infos(
    # image_rgb=image_rgb,
    # masks=masks,
    # pixel_to_meter=0.0025,  # å¯é€‰ï¼šåƒç´ è½¬æ¢ä¸ºç±³çš„æ¯”ä¾‹
    # output_path="grasp_infos_visual.png"
    # )

    # # è¾“å‡ºç»“æœ
    # print("âœ… æŠ“å–ä¿¡æ¯å¦‚ä¸‹ï¼š")
    # for info in grasp_infos:
    #     print(info)
    # for info in grasp_infos:
    #     angle = info["angle_deg"]
    #     quat = grasp_angle_to_quaternion(angle)
    #     print(f"ç‰©ä½“ #{info['index']} å››å…ƒæ•°å§¿æ€: [{quat[0]:.2f}, {quat[1]:.2f}, {quat[2]:.2f}, {quat[3]:.2f}]")


    # print("âœ… SAM2 å¯è§†åŒ–å›¾å·²ä¿å­˜è‡³ï¼š", output_img_path)






    # masks, sam_centers, sam2_result = segment_with_sam2(
    #     image_rgb=image_rgb,
    #     boxes=boxes,
    #     sam_model_path=sam2_model_path,
    #     input_image_path=img_path
    # )

    # # Optional: extract grasp angles and quaternions
    # grasp_infos, grasp_img_path = extract_grasp_infos(
    #     image_rgb=image_rgb,
    #     masks=masks,
    #     pixel_to_meter=0.0025,
    #     output_path=img_path.replace(".png", "_grasp_infos.png")
    # )

    # quaternions = []
    # for info in grasp_infos:
    #     angle_deg = info["angle_deg"]
    #     quat = grasp_angle_to_quaternion(angle_deg)
    #     quaternions.append(quat)
    #     print(f"ğŸ” Object #{info['index']} grasp angle = {angle_deg:.2f}Â°, quaternion = {quat}")

    # # ======================= Step 5: 3D Coordinate Projection =============
    # coords_3d = annotate_and_get_3d_coords(
    #     image_path=img_path,
    #     npz_path=npz_path,
    #     pixel_points=sam_centers  # Prefer sam_centers for precision
    # )

    # # Print coordinates
    # for i, coord in enumerate(coords_3d):
    #     if coord is not None:
    #         print(f"ğŸ§­ Object #{i} 3D position: [{coord[0]:.3f}, {coord[1]:.3f}, {coord[2]:.3f}]")
    #     else:
    #         print(f"âš ï¸ Object #{i} has no valid 3D coordinate")

    # from utils.save_json import save_json_from_detection

    # save_json_from_detection(
    #     yolo_path=yolo_model_path,
    #     class_ids=class_ids,
    #     centers=sam_centers,
    #     coords_3d=coords_3d,
    #     widths=[info["width_m"] for info in grasp_infos],
    #     quaternions=quaternions,
    #     # gripper_opens=[compute_gripper_open_close(info["width_m"])[0] for info in grasp_infos],
    #     # gripper_closes=[compute_gripper_open_close(info["width_m"])[1] for info in grasp_infos],
    #     # output_path=json_path
    # )
