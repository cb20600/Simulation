# pose.py
import os
import sys
import numpy as np
# os.environ['PYOPENGL_PLATFORM'] = 'glx'
# os.environ['PYOPENGL_PLATFORM'] = 'egl'

import genesis as gs
import cv2
from desk4 import create_scene
from utils.yolo_utils import detect_fruits
from utils.sam2_utils import segment_with_sam2
from utils.coordinate import annotate_and_get_3d_coords
from utils.save_json import save_json_from_detection

ENABLE_GUI = False  # é™é»˜æ¨¡å¼
ENABLE_GUI = True  # æ¸²æŸ“æ¨¡å¼

# è·¯å¾„é…ç½®
path_base = "imgs/sim_fruit_from_camera"
img_path = f"{path_base}.png"
npz_path = f"{path_base}.npz"
json_path = f"{path_base}.json"
yolo_path = "yolo_train/train_data_split/runs/detect/train/weights/best.pt"
sam_model_path = "checkpoints/sam2_b.pt"

# âœ… Step 0ï¼šä»¿çœŸå›¾åƒé‡‡é›†
def simulate_and_capture_scene(enable_gui=ENABLE_GUI):
    if enable_gui: 
        print("ğŸ® åˆå§‹åŒ– Genesis åœºæ™¯å¹¶é‡‡é›† RGB/ç‚¹äº‘")
        print("ğŸ–¥ï¸ enable_gui =", enable_gui)
        scene, xarm7, fruits, bins, camera = create_scene(enable_gui=ENABLE_GUI)
        dofs_idx = list(range(7))
        pos_init = np.array([0, 0, 0, 0.9, 0, 0.8, 0])

        for _ in range(300):
            camera.move_to_attach()
            scene.step()

        for _ in range(200):
            xarm7.set_dofs_position(pos_init, dofs_idx)
            scene.step()

        if enable_gui:
            rgb_img, _, _, _ = camera.render(rgb=True)
            rgb_img_bgr = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)
            cv2.imwrite(img_path, rgb_img_bgr)
            print(f"âœ… å›¾åƒä¿å­˜æˆåŠŸï¼š{img_path}")

            pointcloud, mask_idx = camera.render_pointcloud(world_frame=True)[:2]
            mask = np.zeros(pointcloud.shape[:2], dtype=bool)
            mask[mask_idx] = True
            np.savez(npz_path, pointcloud=pointcloud, mask=mask)
            print(f"âœ… ç‚¹äº‘å’Œæ©ç å·²ä¿å­˜ä¸º: {npz_path}")
    else:
        print("ğŸš« GUI æ¨¡å¼å…³é—­ï¼Œè·³è¿‡ç›¸æœºæ¸²æŸ“ä¸ç‚¹äº‘ä¿å­˜")
        return None, None


    return scene, camera


# âœ… ä¸»æµç¨‹
if __name__ == "__main__":
    # Step 0: é‡‡é›†å›¾åƒ
    scene, camera = simulate_and_capture_scene(enable_gui=ENABLE_GUI)

    # Step 1: YOLO æ£€æµ‹
    image_rgb, boxes, yolo_centers, class_ids, yolo_output_path = detect_fruits(img_path, yolo_path)

    # Step 2: SAM2 åˆ†å‰²
    if len(boxes) != 0:
        masks, sam_centers, sm2_output_path = segment_with_sam2(
            image_rgb=image_rgb,
            boxes=boxes,
            sam_model_path=sam_model_path,
            input_image_path=img_path
        )
    else:
        print("âŒ æœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡ï¼Œè·³è¿‡åç»­å¤„ç†")
        exit()

    # Step 3: è®¡ç®— 3D åæ ‡
    centers = sam_centers  # or yolo_centers
    coords_3d = annotate_and_get_3d_coords(img_path, npz_path, centers)

    # Step 4: ä¿å­˜ JSON
    save_json_from_detection(
        yolo_path=yolo_path,
        class_ids=class_ids,
        centers=centers,
        coords_3d=coords_3d,
        output_path=json_path
    )

    print(f"âœ… æµç¨‹å…¨éƒ¨å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {json_path}")

    # Step 5: å¯è§†åŒ–ï¼ˆä»… GUI æ¨¡å¼ä¸‹ï¼‰
    if ENABLE_GUI and scene is not None:
        print("ğŸ‘ï¸ ä¿æŒ Genesis å¯è§†åŒ–çª—å£å¼€å¯ï¼Œå¯æŒ‰ Ctrl+C æ‰‹åŠ¨é€€å‡º")
        while True:
            scene.step()
            camera.render()
